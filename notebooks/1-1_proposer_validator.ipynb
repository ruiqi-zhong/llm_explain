{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_explain.llm.propose import propose, create_proposer_diff_prompt_body\n",
    "from llm_explain.llm.validate import validate, create_prompt_body\n",
    "from llm_explain.models.diff import explain_diff, ExplainDiffResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [\"cat\", \"dog\", \"fish\", \"carrot\", \"potato\", \"apple\"]\n",
    "Y = [False, False, False, True, True, True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposer Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are two sets of text x_samples.\n",
      "\n",
      "Some x_samples from the negative class:\n",
      "Negative class sample.0: cat\n",
      "Negative class sample.1: dog\n",
      "Negative class sample.2: fish\n",
      "\n",
      "Some x_samples from the positive class:\n",
      "Positive class sample.0: carrot\n",
      "Positive class sample.1: potato\n",
      "Positive class sample.2: apple\n",
      "\n",
      "We want to understand what kind of text x_samples are more likely to be in the positive class. Please suggest me at most 3 descriptions. Each of them needs to be a predicate about a text.. Each predicate needs to be wrapped in <predicate> tags.\n",
      "\n",
      "Here are some example predicates:\n",
      "<predicate>uses double negation</predicate>\n",
      "<predicate>has a conservative stance</predicate>\n",
      "\n",
      "Just output the predicates surrounded by <predicate> tags. Do not include any other text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "proposer_prompt = create_proposer_diff_prompt_body(\n",
    "    X,\n",
    "    Y,\n",
    "    num_explanations=3,\n",
    "    precise=False\n",
    ")\n",
    "print(proposer_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed Explanations $\\phi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- is a type of vegetable\n",
      "- is a type of fruit\n",
      "- is related to food\n"
     ]
    }
   ],
   "source": [
    "explanations = propose(x_samples=X, y=Y, task_name=\"diff\", num_explanations=3, precise=False)\n",
    "for e in explanations:\n",
    "    print(\"-\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validator Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "Your job is to validate whether an x_sample surrounded by <x_sample> tags satisfies a predicate surrounds by <predicate> tags. Your output should be a yes or no.\n",
      "\n",
      "\n",
      "<predicate>has a positive sentiment</predicate>\n",
      "<x_sample>this movie is bad</x_sample>\n",
      "<answer>no</answer>\n",
      "\n",
      "<predicate>contains a green object</predicate>\n",
      "<x_sample>the frog is climbing a tree</x_sample>\n",
      "<answer>yes</answer>\n",
      "\n",
      "\n",
      "Now validate the following.\n",
      "\n",
      "<predicate>is a type of vegetable</predicate>\n",
      "<x_sample>cat</x_sample>\n",
      "\n",
      "Just output the answer surrounded by <answer> tags.\n"
     ]
    }
   ],
   "source": [
    "one_sample = X[0]\n",
    "e = explanations[0]\n",
    "\n",
    "prompt = create_prompt_body(predicate=e, x_sample=one_sample)\n",
    "print(\"Prompt:\", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of the validation $[[\\phi]](x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 0\n"
     ]
    }
   ],
   "source": [
    "ans = validate(predicate=e, x_sample=one_sample)\n",
    "print(\"Answer:\", ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at the implementation of the algorithms in explain_diff. Here are the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation: is a type of fruit or vegetable\n",
      "Accuracy: 1.0\n",
      "\n",
      "Explanation: is consumable by humans\n",
      "Accuracy: 0.8333333333333333\n",
      "\n",
      "Explanation: grows in a garden or farm\n",
      "Accuracy: 1.0\n",
      "\n",
      "Explanation: is a type of vegetable or fruit\n",
      "Accuracy: 1.0\n",
      "\n",
      "Explanation: is related to food or agriculture\n",
      "Accuracy: 0.8333333333333333\n",
      "\n",
      "Explanation: is something that grows in soil\n",
      "Accuracy: 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result: ExplainDiffResult = explain_diff(X=X, Y=Y, proposer_num_rounds=2, proposer_num_explanations_per_round=3, proposer_precise=False)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-explain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
