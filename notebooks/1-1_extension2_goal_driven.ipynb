{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.samples import get_goal_driven_examples\n",
    "from llm_explain.llm.propose import create_proposer_diff_prompt_body\n",
    "from llm_explain.models.diff import explain_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing datasets of airline reviews.\n",
    "\n",
    "In this dataset, we are comparing the United Airline (U.S., in English) vs. reviews for Air France (mostly in French), and the goal is to understand the difference in aspect of services.\n",
    "\n",
    "If we do not add any constraints to our system, our system will say that the later \"is more often in French\", which is not very useful.\n",
    "\n",
    "We will show the prompt we used, and the difference between using our system with or without the goal-constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm trying to decide which airline (United or Air France) to fly on, I want to understand the difference between aspects of the service.\n",
      "The predicate should be about aspects of the service, and does NOT mention airline names (United or Air France), positive or negative classes, or language (French or English). Be specific, for example, 'has a positive sentiment' is not a good predicate, but 'complains about flight delays' is a good predicate.\n"
     ]
    }
   ],
   "source": [
    "dataset = get_goal_driven_examples(seed=42)\n",
    "print(dataset[\"context\"])\n",
    "print(dataset[\"constraint\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The prompt that includes the goal\n",
    "\n",
    "(pay attention to the last part of the prompt in the next cell, starting from \"Here is some context about the text x_samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are two sets of text x_samples.\n",
      "\n",
      "Some x_samples from the negative class:\n",
      "Negative class sample.0:  on United Airlines, cabin lighting was well adjusted\n",
      "Negative class sample.1:  on United Airlines, safety instructions were clear\n",
      "Negative class sample.2:  on United Airlines, seat recline mechanism worked smoothly\n",
      "Negative class sample.3:  on United Airlines, plane looked new and modern inside\n",
      "Negative class sample.4:  on United Airlines, temperature remained comfortable\n",
      "Negative class sample.5:  on United Airlines, convenient flight times and connections\n",
      "Negative class sample.6:  on United Airlines, wifi connection was reliable\n",
      "Negative class sample.7:  on United Airlines, plenty of legroom in economy\n",
      "Negative class sample.8:  on United Airlines, minimal turbulence during the flight\n",
      "Negative class sample.9:  on United Airlines, smooth takeoff and landing\n",
      "Negative class sample.10:  on United Airlines, entertainment system worked great\n",
      "Negative class sample.11:  on United Airlines, flight arrived right on schedule\n",
      "Negative class sample.12:  on United Airlines, the flight was on time\n",
      "Negative class sample.13:  on United Airlines, seat belt signs functioned properly\n",
      "\n",
      "Some x_samples from the positive class:\n",
      "Positive class sample.0:  on Air France, les saveurs étaient incroyables\n",
      "Positive class sample.1:  on Air France, ils servent des plats tellement savoureux\n",
      "Positive class sample.2:  on Air France, vraiment satisfait de mon expérience culinaire\n",
      "Positive class sample.3:  on Air France, meilleur repas que j'ai eu depuis longtemps\n",
      "Positive class sample.4:  on Air France, j'ai hâte de goûter plus de leurs plats\n",
      "Positive class sample.5:  on Air France, j'ai apprécié manger ici\n",
      "Positive class sample.6:  on Air France, la nourriture était absolument fantastique\n",
      "Positive class sample.7:  on Air France, la pizza est bonne\n",
      "Positive class sample.8:  on Air France, le dîner est excellent\n",
      "Positive class sample.9:  on Air France, flight attendants went above and beyond\n",
      "Positive class sample.10:  on Air France, l'hôtesse de l'air a fait en sorte que tout le monde se sente bienvenu\n",
      "Positive class sample.11:  on Air France, cabin crew was exceptionally professional\n",
      "Positive class sample.12:  on Air France, representatives accommodated special requests kindly\n",
      "Positive class sample.13:  on Air France, le personnel a géré le retard avec élégance\n",
      "\n",
      "We want to understand what kind of text x_samples are more likely to be in the positive class. Please suggest me at most 3 descriptions. Each of them needs to be a predicate about a text.. Each predicate needs to be wrapped in <predicate> tags.\n",
      "\n",
      "Here are some example predicates:\n",
      "<predicate>uses double negation</predicate>\n",
      "<predicate>has a conservative stance</predicate>\n",
      "\n",
      "Here is some context about the text x_samples: I'm trying to decide which airline (United or Air France) to fly on, I want to understand the difference between aspects of the service..\n",
      "You need to follow the constraint: The predicate should be about aspects of the service, and does NOT mention airline names (United or Air France), positive or negative classes, or language (French or English). Be specific, for example, 'has a positive sentiment' is not a good predicate, but 'complains about flight delays' is a good predicate..\n",
      "Just output the predicates surrounded by <predicate> tags. Do not include any other text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = create_proposer_diff_prompt_body(x_samples=dataset['X'], y=dataset['Y'], constraint=dataset['constraint'], context=dataset['context'], num_explanations=3)\n",
    "print(prompt)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining differences WITH the constraints based on the goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing top 3 explanations:\n",
      "Explanation: comments on the excellence of in-flight hospitality\n",
      "Accuracy: 0.8571428571428572\n",
      "\n",
      "Explanation: expresses satisfaction with the dining experience\n",
      "Accuracy: 0.8214285714285714\n",
      "\n",
      "Explanation: expresses satisfaction with the dining experience\n",
      "Accuracy: 0.8214285714285714\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"proposer_num_rounds\": 5,\n",
    "    \"proposer_num_explanations_per_round\": 3,\n",
    "    \"proposer_precise\": False,\n",
    "    **dataset,\n",
    "}\n",
    "result = explain_diff(**args)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining differences WITHOUT the constraints based on the goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing top 3 explanations:\n",
      "Explanation: uses French language\n",
      "Accuracy: 0.8928571428571428\n",
      "\n",
      "Explanation: includes phrases in French\n",
      "Accuracy: 0.8928571428571428\n",
      "\n",
      "Explanation: is in French\n",
      "Accuracy: 0.8928571428571428\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del dataset[\"constraint\"]\n",
    "del dataset[\"context\"]\n",
    "\n",
    "args = {\n",
    "    \"proposer_num_rounds\": 5,\n",
    "    \"proposer_num_explanations_per_round\": 3,\n",
    "    \"proposer_precise\": False,\n",
    "    **dataset,\n",
    "}\n",
    "result = explain_diff(**args)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-explain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
